#!/usr/bin/env python
# coding: utf-8

# # Clientes existentes automatizado

# ## Instalar librerias

# In[1]:


pip install awswrangler


# In[2]:


get_ipython().system('pip install odfpy')


# In[3]:


pip install boto3 botocore


# ## Importar librerias

# In[4]:


import numpy as np
from datetime import datetime
import awswrangler as wr
import pandas as pd
import calendar
import datetime
from datetime import datetime
from datetime import date
import pytz
from datetime import timedelta
from dateutil.relativedelta import relativedelta
import boto3
from botocore.exceptions import ClientError


# ## Conexion a redshift

# In[5]:


con_redshift_2 = wr.redshift.connect("redshift_cctp_inovation")
conn = wr.redshift.connect("redshift_devsie")


# ## Variables para la creación de las tablas

# ### Obtener fecha del día anterior

# In[6]:


# Set the time zone to Mexico City
mexico_timezone = pytz.timezone('America/Mexico_City')

# Get the current datetime in Mexico City
current_datetime = datetime.now(mexico_timezone)

# Extract the date
current_date = current_datetime.date()
previous_date=current_datetime.date() - relativedelta(days=1)

# Convert the datetime object back to a formatted string
previous_date= previous_date.strftime("%Y%m%d")


# Corvert the datetimes to a formated int 
previous_date = int(previous_date)


# Print the result in a formatted way
print("día vencido:",previous_date )


# ## Validar si en las base black_box.cartera hay información a día vencido

# In[7]:


sql1 = f"""
           select 
                 cuenta,fecha_informacion
           from black_box.cartera
           where fecha_informacion={previous_date}
           limit 100;
               """


# In[8]:


df1 = wr.redshift.read_sql_query(sql1,conn)


# ## Validar si existe la carpeta en S3

# In[9]:


s3_client = boto3.client('s3')


# In[10]:


def check_s3_folder(bucket_name, folder_name):
    try:
        response = s3_client.head_object(Bucket=bucket_name, Key=folder_name + '/')
    except ClientError as e:
        if e.response['Error']['Code'] == '404':
            return False
        else:
            raise
    return True


# In[11]:


bucket_name = 'desarrollos-bigdata-inovacion-us-east-1'
folder_name = f'SAS/sas94/dev_existentes/{previous_date}'


# ## Subir la tabla dev_veces_mora a S3 en formato parquet

# In[12]:


if not df1.empty:
    if not check_s3_folder(bucket_name, folder_name):
        sql = f"""UNLOAD ($$
                                SELECT *,0 AS existentes
                                FROM black_box.cartera AS b
                                WHERE fecha_informacion={previous_date}
                                      and b.cuenta not in (SELECT f.idcuentabrm__c
                                                       FROM credito.cat_ctatelco_ctabank_fiserv AS f)
                                UNION
                                SELECT *,1 AS existentes
                                FROM black_box.cartera AS b
                                WHERE fecha_informacion={previous_date}
                                      and b.cuenta in (SELECT f.idcuentabrm__c
                                                       FROM credito.cat_ctatelco_ctabank_fiserv AS f)
                        $$)
                        TO 's3://desarrollos-bigdata-inovacion-us-east-1/SAS/sas94/dev_existentes/{previous_date}/'
                        IAM_ROLE 'arn:aws:iam::903746939682:role/ReadTP'
                        PARALLEL OFF
                        FORMAT PARQUET
        """
        df = wr.redshift.read_sql_query(sql,conn)
    else:
            print("Ya existe la carpeta en S3.")
else:
    print("No hay información en la base black_box.cartera para el día",previous_date)
